import os
import streamlit as st
import tempfile
import time
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.document_loaders import UnstructuredWordDocumentLoader, TextLoader

# âœ… è®¾ç½® OpenAI API Key
OPENAI_API_KEY = "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"
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# âœ… **Streamlit é¡µé¢é…ç½®**
st.set_page_config(page_title="ğŸš€ Full-Powered GPT", layout="wide")

# âœ… **æ·»åŠ åŠ¨å›¾èƒŒæ™¯**
st.markdown("""
    <style>
    .stApp {
        background: url("https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExYWNzcDE2amJlMXNkNm85amV6azhqN3JiNG16M2g4aHExbmZ2dnFoeSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/7VzgMsB6FLCilwS30v/giphy.gif") no-repeat center center fixed;
        background-size: cover;
    } 
    </style>
""", unsafe_allow_html=True)

st.markdown("""
    <style>
        /* ä¿®æ”¹æ ‡é¢˜é¢œè‰² */
        .stApp h1 {
            color: #FFD700 !important;  /* æ ‡é¢˜å˜æˆé‡‘è‰² */
            text-align: center !important;
        }

        /* ä¿®æ”¹ä¸Šä¼ æ–‡ä»¶çš„æ ‡é¢˜ */
        .stFileUploader label {
            color: #FFFFFF !important;  /* äº®ç´«è‰² */
            font-size: 16px !important;
            font-weight: bold !important;
        }

        /* ä¿®æ”¹å·²ä¸Šä¼ çš„æ–‡ä»¶åé¢œè‰² */
        .uploadedFile {
            color: #FFFFFF !important;  /* æ·±ç°è‰² */
            font-size: 14px !important;
            font-weight: bold !important;
        }
         /* ç”¨æˆ·æé—®æ°”æ³¡ */
        .stChatMessageUser {
            background-color: #3b5998 !important;  /* æ·¡è“è‰² */
            color: #FFFFFF !important;  /* çº¯ç™½å­—ä½“ */
            font-weight: bold !important;
            font-size: 16px !important;
            padding: 12px !important;
            border-radius: 10px !important;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.2) !important;
        }

        /* AI å›å¤æ°”æ³¡ */
        .stChatMessageAssistant {
            background-color: #FFFFFF !important;  /* çº¯ç™½èƒŒæ™¯ */
            color: #000000 !important;  /* é»‘è‰²å­—ä½“ */
            font-weight: bold !important;
            font-size: 16px !important;
            padding: 12px !important;
            border-radius: 10px !important;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.2) !important;
        }
    </style>
    """, unsafe_allow_html=True)

st.markdown("""
    <style>
        /* ä¿®æ”¹æç¤ºä¿¡æ¯ (st.info) çš„é¢œè‰² */
        .stAlert {
            color: #FFFFFF !important;  /* æ–‡å­—å˜æˆç™½è‰² */
            font-weight: bold !important;
            font-size: 16px !important;
        }
    </style>
""", unsafe_allow_html=True)
st.markdown(
    """
    <style>
        /* ä¿®æ”¹ä¸Šä¼ åæ–‡ä»¶åçš„é¢œè‰² */
        .stUploadedFile {
            color: #FFFFFF !important;  /* è¿™é‡Œæ”¹æˆä½ æƒ³è¦çš„é¢œè‰²ï¼Œæ¯”å¦‚ç™½è‰² */
            font-size: 14px !important;
            font-weight: bold !important;
        }
    </style>
    """,
    unsafe_allow_html=True
)

# âœ… **æ ‡é¢˜**
st.title("ğŸ’¬ Chat with Documents")

# âœ… **æ–‡ä»¶ä¸Šä¼ **
uploaded_files = st.file_uploader("ğŸ“‚ Upload documents (PDF, DOCX, TXT)", accept_multiple_files=True, type=["pdf", "docx", "txt"])

if uploaded_files:
    documents = []

    # âœ… **åˆå§‹åŒ–å‘é‡å­˜å‚¨**
    if "vector_store" not in st.session_state:
        with st.spinner("ğŸ”„ Processing your documents..."):
            with tempfile.TemporaryDirectory() as temp_dir:
                for file in uploaded_files:
                    temp_file_path = os.path.join(temp_dir, file.name)
                    with open(temp_file_path, "wb") as f:
                        f.write(file.getbuffer())

                    # é€‰æ‹©åŠ è½½å™¨
                    if file.name.endswith(".pdf"):
                        loader = PyPDFLoader(temp_file_path)
                    elif file.name.endswith(".docx"):
                        loader = UnstructuredWordDocumentLoader(temp_file_path)
                    elif file.name.endswith(".txt"):
                        loader = TextLoader(temp_file_path)
                    else:
                        continue

                    documents.extend(loader.load())

                # **æ–‡æœ¬åˆ†å—**
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                docs = text_splitter.split_documents(documents)

                # **ç”ŸæˆåµŒå…¥å¹¶å­˜å…¥ FAISS**
                embeddings = OpenAIEmbeddings()
                st.session_state.vector_store = FAISS.from_documents(docs, embeddings)

        st.success("âœ… Documents processed! You can start chatting.")

    # âœ… **åˆå§‹åŒ–èŠå¤©è®°å½•**
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # âœ… **æ˜¾ç¤ºèŠå¤©å†å²**
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # âœ… **ç”¨æˆ·è¾“å…¥**
    user_input = st.chat_input("ğŸ’¬ Ask a question about your documents...")

    if user_input:
        # ğŸ¤ **æ·»åŠ ç”¨æˆ·æ¶ˆæ¯**
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # ğŸ”— **åˆ›å»ºå¯¹è¯é“¾**
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        retriever = st.session_state.vector_store.as_retriever()
        
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(model_name="gpt-4o", temperature=0.2, max_tokens=512),
            retriever=retriever,
            memory=memory
        )

        # ğŸ¤– **ç”Ÿæˆå›ç­”**
        with st.spinner("ğŸ¤– Thinking..."):
            response = qa_chain.invoke({"question": user_input})
            response_text = response["answer"]

         # â³ **æµå¼æ˜¾ç¤º AI å“åº”**
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            for chunk in response_text:
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")
                time.sleep(0.05)
            message_placeholder.markdown(full_response)

        # ğŸ’¾ **ä¿å­˜æ¶ˆæ¯è®°å½•**
        st.session_state.messages.append({"role": "assistant", "content": response_text})
